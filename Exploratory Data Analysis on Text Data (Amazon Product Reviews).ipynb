{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas_profiling'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minteractiveshell\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InteractiveShell\n\u001b[0;32m      9\u001b[0m InteractiveShell\u001b[38;5;241m.\u001b[39mast_node_interactivity \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas_profiling\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas_profiling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ProfileReport\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas_profiling'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# For visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import pandas_profiling\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "import re\n",
    "# For handling string\n",
    "import string\n",
    "# For performing mathematical operations\n",
    "import math\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing dataset\n",
    "df=pd.read_csv('Datafiniti_Amazon.csv') \n",
    "print(\"Shape of data=>\",df.shape)\n",
    "\n",
    "#Reference:https://www.analyticsvidhya.com/blog/2020/04/beginners-guide-exploratory-data-analysis-text-data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df)\n",
    "df.shape\n",
    "df.info()\n",
    "df.describe().transpose()\n",
    "df.head(n=20)\n",
    "df.tail()\n",
    "pd.isna(df)\n",
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting columns\n",
    "df=df[['name','reviews.text','reviews.doRecommend','reviews.numHelpful']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for null values \n",
    "df.dropna(inplace=True)\n",
    "df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word Count of each review\n",
    "df['word_count'] = df['reviews.text'].apply(lambda x: len(str(x).split(\" \")))\n",
    "df[['reviews.text','word_count']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average word count of each review\n",
    "\n",
    "sum(df['word_count'])/len(df['reviews.text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Character Count of each review\n",
    "df['char_count'] = df['reviews.text'].str.len() ## this also includes spaces\n",
    "df[['reviews.text','char_count']].head()\n",
    "plt.hist(df['char_count'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average Length of words in Review 0 \n",
    "sum(len(df['reviews.text'][0]) for word in df['reviews.text'][0])/len(df['reviews.text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average word size of each review\n",
    "\n",
    "def avg_word(sentence):\n",
    "  words = sentence.split()\n",
    "  return (sum(len(word) for word in words)/len(words))\n",
    "\n",
    "df['avg_word'] = df['reviews.text'].apply(lambda x: avg_word(x))\n",
    "df[['reviews.text','avg_word']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['name'].unique()\n",
    "print('Number of products=>',len(df['name'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep products which have a count more than 500 \n",
    "df=df.groupby('name').filter(lambda x:len(x)>500).reset_index(drop=True)\n",
    "print('Number of products=>',len(df['name'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert columns to int so they can be used to build a model later \n",
    "df['reviews.doRecommend']=df['reviews.doRecommend'].astype(int)\n",
    "df['reviews.numHelpful']=df['reviews.numHelpful'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['name'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove ,,, from entries in name column\n",
    "df['name'] = df['name'].str.replace(r'\\,,,','')\n",
    "\n",
    "df['name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check reviews. text column\n",
    "print(df['reviews.text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check first 30 reviews \n",
    "i=0\n",
    "while i<50:\n",
    "    print (\"Review Number \",i)\n",
    "    print()\n",
    "    print(df['reviews.text'][i])\n",
    "    print()\n",
    "    i=i+1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove contractions from text reviews\n",
    "# Dictionary of English Contractions\n",
    "contractions_dict = { \"It's\":\"It is\",\"it's\":\"it is\", \"ain't\": \"are not\",\"'s\":\" is\",\"aren't\": \"are not\",\n",
    "                     \"can't\": \"cannot\",\"can't've\": \"cannot have\",\n",
    "                     \"'cause\": \"because\",\"could've\": \"could have\",\"couldn't\": \"could not\",\n",
    "                     \"couldn't've\": \"could not have\", \"didn't\": \"did not\",\"doesn't\": \"does not\",\n",
    "                     \"don't\": \"do not\",\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\n",
    "                     \"hasn't\": \"has not\",\"haven't\": \"have not\",\"he'd\": \"he would\",\n",
    "                     \"he'd've\": \"he would have\",\"he'll\": \"he will\", \"he'll've\": \"he will have\",\n",
    "                     \"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\n",
    "                     \"I'd\": \"I would\", \"I'd've\": \"I would have\",\"I'll\": \"I will\",\n",
    "                     \"I'll've\": \"I will have\",\"I'm\": \"I am\",\"I've\": \"I have\", \"isn't\": \"is not\",\n",
    "                     \"it'd\": \"it would\",\"it'd've\": \"it would have\",\"it'll\": \"it will\",\n",
    "                     \"it'll've\": \"it will have\", \"let's\": \"let us\",\"ma'am\": \"madam\",\n",
    "                     \"mayn't\": \"may not\",\"might've\": \"might have\",\"mightn't\": \"might not\", \n",
    "                     \"mightn't've\": \"might not have\",\"must've\": \"must have\",\"mustn't\": \"must not\",\n",
    "                     \"mustn't've\": \"must not have\", \"needn't\": \"need not\",\n",
    "                     \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\"oughtn't\": \"ought not\",\n",
    "                     \"oughtn't've\": \"ought not have\",\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\n",
    "                     \"shan't've\": \"shall not have\",\"she'd\": \"she would\",\"she'd've\": \"she would have\",\n",
    "                     \"she'll\": \"she will\", \"she'll've\": \"she will have\",\"should've\": \"should have\",\n",
    "                     \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\",\"so've\": \"so have\",\n",
    "                     \"that'd\": \"that would\",\"that'd've\": \"that would have\", \"there'd\": \"there would\",\n",
    "                     \"there'd've\": \"there would have\", \"they'd\": \"they would\",\n",
    "                     \"they'd've\": \"they would have\",\"they'll\": \"they will\",\n",
    "                     \"they'll've\": \"they will have\", \"they're\": \"they are\",\"they've\": \"they have\",\n",
    "                     \"to've\": \"to have\",\"wasn't\": \"was not\",\"we'd\": \"we would\",\n",
    "                     \"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\n",
    "                     \"we're\": \"we are\",\"we've\": \"we have\", \"weren't\": \"were not\",\"what'll\": \"what will\",\n",
    "                     \"what'll've\": \"what will have\",\"what're\": \"what are\", \"what've\": \"what have\",\n",
    "                     \"when've\": \"when have\",\"where'd\": \"where did\", \"where've\": \"where have\",\n",
    "                     \"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who've\": \"who have\",\n",
    "                     \"why've\": \"why have\",\"will've\": \"will have\",\"won't\": \"will not\",\n",
    "                     \"won't've\": \"will not have\", \"would've\": \"would have\",\"wouldn't\": \"would not\",\n",
    "                     \"wouldn't've\": \"would not have\",\"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
    "                     \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\n",
    "                     \"y'all've\": \"you all have\", \"you'd\": \"you would\",\"you'd've\": \"you would have\",\n",
    "                     \"you'll\": \"you will\",\"you'll've\": \"you will have\", \"you're\": \"you are\",\n",
    "                     \"you've\": \"you have\"}\n",
    "\n",
    "# Regular expression for finding contractions\n",
    "contractions_re=re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n",
    "\n",
    "# Function for expanding contractions\n",
    "def expand_contractions(text,contractions_dict=contractions_dict):\n",
    "  def replace(match):\n",
    "    return contractions_dict[match.group(0)]\n",
    "  return contractions_re.sub(replace, text)\n",
    "\n",
    "# Expanding Contractions in the reviews\n",
    "df['reviews.text']=df['reviews.text'].apply(lambda x:expand_contractions(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check first 30 reviews \n",
    "i=0\n",
    "while i<50:\n",
    "    print (\"Review Number \",i)\n",
    "    print()\n",
    "    print(df['reviews.text'][i])\n",
    "    print()\n",
    "    i=i+1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['reviews.text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['reviews.text']=df['reviews.text'].str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alternative to lowercase letters\n",
    "df['cleaned']=df['reviews.text'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove digits and words containing digits \n",
    "df['reviews.text']=df['reviews.text'].apply(lambda x: re.sub('\\w*\\d\\w*','', x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Punctuations\n",
    "df['reviews.text']=df['reviews.text'].apply(lambda x: re.sub('[%s]' % re.escape(string.punctuation), '', x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check first 30 reviews after cleaning \n",
    "i=0\n",
    "while i<50:\n",
    "    print (\"Review Number \",i)\n",
    "    print()\n",
    "    print(df['reviews.text'][i])\n",
    "    print()\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z=df['reviews.text']\n",
    "print(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check count of StopWords\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "df['stopwords'] = df['reviews.text'].apply(lambda x: len([x for x in x.split() if x in stop]))\n",
    "df[['reviews.text','stopwords']].head()\n",
    "\n",
    "plt.hist(df['stopwords'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Commonly occuring words\n",
    "freq = pd.Series(' '.join(df['reviews.text']).split()).value_counts()[:10]\n",
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removal of commonly occuring irrelevant words\n",
    "freq = list(freq)\n",
    "df['reviews.text'] = df['reviews.text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "df['reviews.text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count of rare words\n",
    "\n",
    "freq_rare = pd.Series(' '.join(df['reviews.text']).split()).value_counts()[-100:]\n",
    "freq_rare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removal of rarely occuring irrelevant words\n",
    "freq_rare = list(freq_rare)\n",
    "df['reviews.text'] = df['reviews.text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "df['reviews.text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spell Correction(Time consuming), Try on first 5 reviews\n",
    "from textblob import TextBlob\n",
    "df['reviews.text_Corrected Spelling']=df['reviews.text'][1:5].apply(lambda x: str(TextBlob(x).correct()))\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import stopwords with nltk.\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "# Exclude stopwords with Python's list comprehension and pandas.DataFrame.apply.\n",
    "df['reviews.text.nostopwords'] = df['reviews.text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n",
    "\n",
    "df['reviews.text.nostopwords.lemmatized2'] = df['reviews.text.nostopwords'].apply(lambda x:lemmatize_text(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install textblob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Counting number of words using TextBlob\n",
    "from textblob import TextBlob\n",
    "TextBlob(df['reviews.text.nostopwords'][1]).word_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatization\n",
    "\n",
    "from textblob import Word\n",
    "df['reviews.text.nostopwords.LEMMATIZED'] = df['reviews.text.nostopwords'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "df['reviews.text.nostopwords.LEMMATIZED'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n-gram using TextBlob\n",
    "\n",
    "TextBlob(df['reviews.text.nostopwords.LEMMATIZED'][0]).ngrams(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scikit learn implementation(Tfidf vectorizer)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TFID BOW Vectorizer\n",
    "\n",
    "vec_idf=TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting the vectorizer on the data to create vocabulary\n",
    "\n",
    "w=vec_idf.fit(df['reviews.text.nostopwords.LEMMATIZED'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vocabulary \n",
    "\n",
    "vec_idf.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TFIDF Score calculation\n",
    "#Output would be sparse matrix with TFIDF score calculation \n",
    "\n",
    "z=w.transform(df['reviews.text.nostopwords.LEMMATIZED'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting sparse matrix into array\n",
    "\n",
    "print(z.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Term Matrix (Another Approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group stopwords and lemmatized reviews according to products\n",
    "df_grouped=df[['name','reviews.text.nostopwords.LEMMATIZED']].groupby(by='name').agg(lambda x:' '.join(x))\n",
    "df_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Document Term Matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv=CountVectorizer(analyzer='word')\n",
    "data=cv.fit_transform(df_grouped['reviews.text.nostopwords.LEMMATIZED'])\n",
    "df_dtm = pd.DataFrame(data.toarray(), columns=cv.get_feature_names())\n",
    "df_dtm.index=df_grouped.index\n",
    "df_dtm.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dtm.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing wordcloud for plotting word clouds and textwrap for wrapping longer text\n",
    "from wordcloud import WordCloud\n",
    "from textwrap import wrap\n",
    "\n",
    "# Function for generating word clouds\n",
    "def generate_wordcloud(data,title):\n",
    "  wc = WordCloud(width=400, height=330, max_words=150,colormap=\"Dark2\").generate_from_frequencies(data)\n",
    "  plt.figure(figsize=(10,8))\n",
    "  plt.imshow(wc, interpolation='bilinear')\n",
    "  plt.axis(\"off\")\n",
    "  plt.title('\\n'.join(wrap(title,60)),fontsize=13)\n",
    "  plt.show()\n",
    "  \n",
    "# Transposing document term matrix\n",
    "df_dtm=df_dtm.transpose()\n",
    "\n",
    "# Plotting word cloud for each product\n",
    "for index,product in enumerate(df_dtm.columns):\n",
    "  generate_wordcloud(df_dtm[product].sort_values(ascending=False),product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "df['polarity']=df['reviews.text.nostopwords.LEMMATIZED'].apply(lambda x:TextBlob(x).sentiment.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reviews with Highest Polarity \n",
    "print(\"3 Random Reviews with Highest Polarity:\")\n",
    "for index,review in enumerate(df.iloc[df['polarity'].sort_values(ascending=False)[:3].index]['reviews.text']):\n",
    "  print('Review {}:\\n'.format(index+1),review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reviews with Lowest Polarity \n",
    "print(\"3 Random Reviews with Lowest Polarity:\")\n",
    "for index,review in enumerate(df.iloc[df['polarity'].sort_values(ascending=True)[:3].index]['reviews.text']):\n",
    "  print('Review {}:\\n'.format(index+1),review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting Polarity of different products\n",
    "product_polarity_sorted=pd.DataFrame(df.groupby('name')['polarity'].mean().sort_values(ascending=True))\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.xlabel('Polarity')\n",
    "plt.ylabel('Products')\n",
    "plt.title('Polarity of Different Amazon Product Reviews')\n",
    "polarity_graph=plt.barh(np.arange(len(product_polarity_sorted.index)),product_polarity_sorted['polarity'],color='purple',)\n",
    "\n",
    "# Writing product names on bar\n",
    "for bar,product in zip(polarity_graph,product_polarity_sorted.index):\n",
    "  plt.text(0.005,bar.get_y()+bar.get_width(),'{}'.format(product),va='center',fontsize=11,color='white')\n",
    "\n",
    "# Writing polarity values on graph\n",
    "for bar,polarity in zip(polarity_graph,product_polarity_sorted['polarity']):\n",
    "  plt.text(bar.get_width()+0.001,bar.get_y()+bar.get_width(),'%.3f'%polarity,va='center',fontsize=11,color='black')\n",
    "  \n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting recommendation of different products\n",
    "recommend_percentage=pd.DataFrame(((df.groupby('name')['reviews.doRecommend'].sum()*100)/df.groupby('name')['reviews.doRecommend'].count()).sort_values(ascending=True))\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.xlabel('Recommend Percentage')\n",
    "plt.ylabel('Products')\n",
    "plt.title('Percentage of reviewers recommended a product')\n",
    "recommend_graph=plt.barh(np.arange(len(recommend_percentage.index)),recommend_percentage['reviews.doRecommend'],color='green')\n",
    "\n",
    "# Writing product names on bar\n",
    "for bar,product in zip(recommend_graph,recommend_percentage.index):\n",
    "  plt.text(0.5,bar.get_y()+0.4,'{}'.format(product),va='center',fontsize=11,color='white')\n",
    "\n",
    "# Writing recommendation percentage on graph\n",
    "for bar,percentage in zip(recommend_graph,recommend_percentage['reviews.doRecommend']):\n",
    "  plt.text(bar.get_width()+0.5,bar.get_y()+0.4,'%.2f'%percentage,va='center',fontsize=11,color='black')\n",
    "\n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking readability of reviews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using textstat to determine readability, complexity and grade level of particular review\n",
    "import textstat\n",
    "df['dale_chall_score']=df['reviews.text'].apply(lambda x: textstat.dale_chall_readability_score(x))\n",
    "df['flesh_reading_ease']=df['reviews.text'].apply(lambda x: textstat.flesch_reading_ease(x))\n",
    "df['gunning_fog']=df['reviews.text'].apply(lambda x: textstat.gunning_fog(x))\n",
    "\n",
    "print('Dale Chall Score of upvoted reviews=>',df[df['reviews.numHelpful']>1]['dale_chall_score'].mean())\n",
    "print('Dale Chall Score of not upvoted reviews=>',df[df['reviews.numHelpful']<=1]['dale_chall_score'].mean())\n",
    "\n",
    "print('Flesch Reading Score of upvoted reviews=>',df[df['reviews.numHelpful']>1]['flesh_reading_ease'].mean())\n",
    "print('Flesch Reading Score of not upvoted reviews=>',df[df['reviews.numHelpful']<=1]['flesh_reading_ease'].mean())\n",
    "\n",
    "print('Gunning Fog Index of upvoted reviews=>',df[df['reviews.numHelpful']>1]['gunning_fog'].mean())\n",
    "print('Gunning Fog Index of not upvoted reviews=>',df[df['reviews.numHelpful']<=1]['gunning_fog'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking if upvoted and not upvoted reviews are easility understandable by people who completed certain grade school\n",
    "df['text_standard']=df['reviews.text'].apply(lambda x: textstat.text_standard(x))\n",
    "\n",
    "print('Text Standard of upvoted reviews=>',df[df['reviews.numHelpful']>1]['text_standard'].mode())\n",
    "print('Text Standard of not upvoted reviews=>',df[df['reviews.numHelpful']<=1]['text_standard'].mode())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking reading time for a review (Average adults reads 250 words per minute)\n",
    "\n",
    "df['reading_time']=df['reviews.text'].apply(lambda x: textstat.reading_time(x))\n",
    "\n",
    "print('Reading Time of upvoted reviews=>',df[df['reviews.numHelpful']>1]['reading_time'].mean())\n",
    "print('Reading Time of not upvoted reviews=>',df[df['reviews.numHelpful']<=1]['reading_time'].mean())\n",
    "\n",
    "#People find longer reviews helpful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conclusion from EDA on Amazon Product Revies\n",
    "\n",
    "#Customers love products from Amazon. They find them a great purchase and easy to use\n",
    "#Amazon needs to work on the Fire Kids Edition Tablet because it has the most negative reviews. It is also the least recommended product\n",
    "#The majority of reviews are written in simple English and are easily understandable by anyone who has 5th or 6th grade of school\n",
    "#The reading time of helpful reviews is twice that of non-helpful reviews which means people find longer reviews helpful\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
